{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script implements a basic character-level sequence-to-sequence model to translate translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\").\n",
    "\n",
    "#### The basic steps of the algorithm are:\n",
    "- start with input sequences from a domain (human readable dates)\n",
    "    and the corresponding target sequences from another domain\n",
    "    (dates in a standard format).\n",
    "- An encoder LSTM turns input sequences into two state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    The decoder uses as initial state the state vectors returned by the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors.\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "        (in fact, a distribution for the next char is generated)\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence.\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "\n",
    "# Data\n",
    "Dates are randomly generated in the load_dataset() function.\n",
    "\n",
    "\n",
    "# References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.1078'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 16431.80it/s]\n"
     ]
    }
   ],
   "source": [
    "#Load dates datasets\n",
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30) -> the maximum length of a phrase is set to 30\n",
      "Y.shape: (10000, 10) -> the output date in standardized format have lenght=10\n",
      "Xoh.shape: (10000, 30, 37) -> the human vocab contains 37 chars\n",
      "Yoh.shape: (10000, 10, 11) -> the machine vocab contains 11 chars\n"
     ]
    }
   ],
   "source": [
    "#[CHECK]\n",
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape, \"-> the maximum length of a phrase is set to 30\")\n",
    "print(\"Y.shape:\", Y.shape,  \"-> the output date in standardized format have lenght=10\")\n",
    "print(\"Xoh.shape:\", Xoh.shape, \"-> the human vocab contains 37 chars\")\n",
    "print(\"Yoh.shape:\", Yoh.shape,\"-> the machine vocab contains 11 chars\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "human_vocab =   {' ': 0,\n",
    "                 '.': 1,\n",
    "                 '/': 2,\n",
    "                 '0': 3,\n",
    "                 '1': 4,\n",
    "                 (...)\n",
    "                 '9': 12,\n",
    "                 'a': 13,\n",
    "                 'b': 14,\n",
    "                 (...)\n",
    "                 'v': 32,\n",
    "                 'w': 33,\n",
    "                 'y': 34,\n",
    "                 '<unk>': 35,\n",
    "                 '<pad>': 36}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "machine_vocab= {'-': 0,\n",
    "               '0': 1,\n",
    "               '1': 2,\n",
    "               '2': 3,\n",
    "               '3': 4,\n",
    "               '4': 5,\n",
    "               '5': 6,\n",
    "               '6': 7,\n",
    "               '7': 8,\n",
    "               '8': 9,\n",
    "               '9': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in human_vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two additional chars to the dict: a start char and an end char\n",
    "reverse_target_char_index = {}\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in machine_vocab.items())\n",
    "reverse_target_char_index.update({11:'<start>', 12:'<end>'})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "reverse_target_char_index:\n",
    "{0: '-', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: '<start>', 12: '<end>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Y_oh to build decoder_input_data and decoder_output_data from Y_oh\n",
    "num_samples,_,_ = Yoh.shape\n",
    "Tx_decoder = 11 #it's 10 (length of YYYY-MM-DD) plus addition char \n",
    "num_decoder_tokens = len(reverse_target_char_index) # return 13\n",
    "decoder_input_data = np.zeros([num_samples, Tx_decoder, num_decoder_tokens])\n",
    "decoder_target_data = np.zeros([num_samples, Tx_decoder, num_decoder_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# encoder input data\n",
    "#####################\n",
    "encoder_input_data = Xoh\n",
    "\n",
    "####################\n",
    "# decoder_input_data\n",
    "####################\n",
    "# timestep = O : populate with start\n",
    "oh_start_char = np.zeros(num_decoder_tokens)\n",
    "oh_start_char[11] = 1 # index 11 corresponds to '<start>'\n",
    "#for i in range(num_samples):\n",
    "decoder_input_data[:, 0] = oh_start_char # all the exemples get a <start> char character\n",
    "\n",
    "for i in range(num_samples):\n",
    "    for j in range(0,Tx_decoder-1): # j=1,...10\n",
    "        for k in range(num_decoder_tokens - 2): # minus 2 because of the <start> ad <end> chars we added\n",
    "            decoder_input_data[i][j+1][k] = Yoh[i][j][k]\n",
    "            \n",
    "#####################\n",
    "# decoder_target_data\n",
    "#####################\n",
    "\n",
    "oh_end_char = np.zeros(num_decoder_tokens)\n",
    "oh_end_char[12] = 1 # index 12 corresponds to '<end>'\n",
    "decoder_target_data[:, Tx_decoder-1] = oh_end_char # targets get an <end> char in the last timestep\n",
    "\n",
    "for i in range(num_samples):\n",
    "    for j in range(0, Tx_decoder-1):\n",
    "        for k in range(num_decoder_tokens):\n",
    "            # decoder_target_data is one time step ahead of the decoder_input_data\n",
    "            decoder_target_data[i][j][k] = decoder_input_data[i][j+1][k] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]] \n",
      " 27 may 2014<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "def from_oh_to_chars(matrix, reverse_dictionary):\n",
    "    # Take a one hot enconding two dimensional ndarray and\n",
    "    # translate it back to human language (phrase)\n",
    "    tx, dim = matrix.shape\n",
    "    resu = str()\n",
    "    for i in range(tx):\n",
    "            if len(np.where(matrix[i]==1)[0])==0:\n",
    "                break\n",
    "            else:\n",
    "                index = np.where(matrix[i]==1)[0][0]\n",
    "                resu+=(reverse_dictionary[index])\n",
    "    return resu\n",
    "\n",
    "print(encoder_input_data[1],'\\n', from_oh_to_chars(encoder_input_data[1], reverse_input_char_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 31 17 29 16 13 34  0 24 13 28 15 20  0  5 11  0  4 12 12  8 36 36 36\n",
      " 36 36 36 36 36 36]  \n",
      "is decoded to \n",
      " tuesday march 28 1995<pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "def from_encode_to_chars(vector_indices, reverse_input_char_index):\n",
    "    resu = ''   \n",
    "    for _,index in enumerate(vector_indices):\n",
    "        resu+=reverse_input_char_index[index]\n",
    "    return resu\n",
    "\n",
    "print(X[0], \" \\nis decoded to \\n\", from_encode_to_chars(X[0], reverse_input_char_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_string(phrase, human_vocab, Tx):\n",
    "    # take string and return a list with the index of each char (in the dictionary human vocab)\n",
    "    # take phrase 'foo' and return [18,26,26]\n",
    "    resu = np.zeros(Tx, dtype=np.int8)\n",
    "    for idx,char in enumerate(phrase):\n",
    "        resu[idx] = human_vocab.get(char)\n",
    "        idx=idx+1\n",
    "    resu[idx:] = human_vocab.get('<pad>')\n",
    "    return resu\n",
    "\n",
    "def from_encode_to_oh(encoded_phrase_indices, reverse_input_char_index):\n",
    "    cols = encoded_phrase_indices\n",
    "    matrix = np.zeros((len(encoded_phrase_indices) , len(reverse_input_char_index) ) )\n",
    "    matrix[np.arange(len(encoded_phrase_indices)) ,cols ] = 1\n",
    "    return matrix\n",
    "\n",
    "def from_nl_to_oh(phrase,human_vocab,reverse_input_char_index, Tx):\n",
    "    encoded_phrase_indices = encode_string(phrase, human_vocab, Tx)\n",
    "    resu = from_encode_to_oh(encoded_phrase_indices,reverse_input_char_index)\n",
    "    return resu\n",
    "\n",
    "from_nl_to_oh('20 may 1998', human_vocab, reverse_input_char_index, Tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 9 may 1998\n",
      "tuesday march 28 1995<pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<start>1995-03-28\n",
      "1995-03-28<end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 9 may 1998\")\n",
    "print(from_encode_to_chars(X[0], reverse_input_char_index))\n",
    "print(from_oh_to_chars(decoder_input_data[0], reverse_target_char_index)) # '<start>1998-05-09'\n",
    "print(from_oh_to_chars(decoder_target_data[0], reverse_target_char_index)) # '1998-05-09<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 10.09.70\n",
      "27 may 2014<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<start>2014-05-27\n",
      "2014-05-27<end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 10.09.70\")\n",
    "print(from_encode_to_chars(X[1], reverse_input_char_index))\n",
    "print(from_oh_to_chars(decoder_input_data[1], reverse_target_char_index)) # '<start>1998-05-09'\n",
    "print(from_oh_to_chars(decoder_target_data[1], reverse_target_char_index)) # '1998-05-09<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: sunday may 22 1988\n",
      "october 22 1979<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<start>1979-10-22\n",
      "1979-10-22<end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Example: sunday may 22 1988\")\n",
    "print(from_encode_to_chars(X[5], reverse_input_char_index))\n",
    "print(from_oh_to_chars(decoder_input_data[5], reverse_target_char_index)) # '<start>1998-05-09'\n",
    "print(from_oh_to_chars(decoder_target_data[5], reverse_target_char_index)) # '1998-05-09<end>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#from keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metaparams\n",
    "batch_size = 64 # 64  # Batch size for training.\n",
    "epochs =  50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = num_samples  # Number of samples to train on.\n",
    "\n",
    "num_encoder_tokens = len(human_vocab)\n",
    "num_decoder_tokens = num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm_decoder =  LSTM(latent_dim, return_sequences=True, return_state=True, name='lstm_decoder')\n",
    "# Let's define here the building blocks of our models\n",
    "lstm_encoder_layer = LSTM(latent_dim, return_state=True, name='lstm_encoder')\n",
    "lstm_decoder_layer =  LSTM(latent_dim, return_sequences=True, return_state=True, name='lstm_decoder')\n",
    "dense_layer = Dense(num_decoder_tokens, activation='softmax', name=\"decoder_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Define model for training\n",
    "##########################\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "def get_training_model(lstm_encoder_layer, lstm_decoder_layer, dense_layer):\n",
    "    \n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_inputs')\n",
    "    \n",
    "    # The input sequence is encoded; the resulting state vectors are kept; \n",
    "    # encoder_outputs, state_h, state_c are tensors\n",
    "    encoder_outputs, state_h, state_c = lstm_encoder_layer(encoder_inputs)\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "    \"\"\"\n",
    "    We set our decoder up to return full output sequences,\n",
    "    and to return internal states as well. We don't use the\n",
    "    return states in the training model, but we will use them \n",
    "    in the inference phase.\n",
    "    \"\"\"\n",
    "    decoder_outputs_0, _, _ = lstm_decoder_layer(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_outputs_1 = dense_layer(decoder_outputs_0)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` and `decoder_input_data` into `decoder_target_data`\n",
    "    training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs_1)\n",
    "    return training_model\n",
    "\n",
    "training_model = get_training_model(lstm_encoder_layer, lstm_decoder_layer, dense_layer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encoder.input returns:  Tensor(\"encoder_inputs_4:0\", shape=(?, ?, 37), dtype=float32)\n",
    "encoder.output returns: [<tf.Tensor 'lstm_encoder_4/TensorArrayReadV3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_encoder_4/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_encoder_4/while/Exit_3:0' shape=(?, 256) dtype=float32>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'lstm_decoder',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'return_sequences': True,\n",
       " 'return_state': True,\n",
       " 'go_backwards': False,\n",
       " 'stateful': False,\n",
       " 'unroll': False,\n",
       " 'units': 256,\n",
       " 'activation': 'tanh',\n",
       " 'recurrent_activation': 'hard_sigmoid',\n",
       " 'use_bias': True,\n",
       " 'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "  'config': {'scale': 1.0,\n",
       "   'mode': 'fan_avg',\n",
       "   'distribution': 'uniform',\n",
       "   'seed': None,\n",
       "   'dtype': 'float32'}},\n",
       " 'recurrent_initializer': {'class_name': 'Orthogonal',\n",
       "  'config': {'gain': 1.0, 'seed': None, 'dtype': 'float32'}},\n",
       " 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}},\n",
       " 'unit_forget_bias': True,\n",
       " 'kernel_regularizer': None,\n",
       " 'recurrent_regularizer': None,\n",
       " 'bias_regularizer': None,\n",
       " 'activity_regularizer': None,\n",
       " 'kernel_constraint': None,\n",
       " 'recurrent_constraint': None,\n",
       " 'bias_constraint': None,\n",
       " 'dropout': 0.0,\n",
       " 'recurrent_dropout': 0.0,\n",
       " 'implementation': 1}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_decoder_layer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None, 37)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None, 13)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_encoder (LSTM)             [(None, 256), (None, 301056      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_decoder (LSTM)             [(None, None, 256),  276480      input_3[0][0]                    \n",
      "                                                                 lstm_encoder[0][1]               \n",
      "                                                                 lstm_encoder[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 13)     3341        lstm_decoder[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 580,877\n",
      "Trainable params: 580,877\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Run training\n",
    "##############\n",
    "TRAINING = False\n",
    "if TRAINING:\n",
    "    training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "    validation_split=0.2)\n",
    "    # Save model\n",
    "    training_model.save('s2s_dates_translator.h5')\n",
    "else:\n",
    "    training_model.load_weights('s2s_dates_translator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Voici the steps:\n",
    "# 1) encode input and retain output as initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "def sampling_model(latent_dim, lstm_encoder_layer, lstm_decoder_layer,decoder_dense):\n",
    "    # Define sampling models\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_inputs')\n",
    "    encoder_outputs, state_h, state_c = lstm_encoder_layer(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and keep only the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,), name='decoder_state_input_h')\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,), name='decoder_state_input_c')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    # This is the lstm we defined before \n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "    decoder_outputs, state_h, state_c = lstm_decoder_layer(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs, # the input is a list containing three tensors\n",
    "        [decoder_outputs] + decoder_states) # the input is also a list containing tensors\n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model, decoder_model = sampling_model(latent_dim, lstm_encoder_layer, lstm_decoder_layer, dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_v = encoder_model.predict(encoder_input_data[2:3])\n",
    "target_seq = np.zeros((1, 11, num_decoder_tokens))\n",
    "target_seq[0, 0, np.where(oh_start_char==1)[0][0]] = 1.\n",
    "outs = decoder_model.predict([target_seq]+states_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "acum=[]\n",
    "for t in range(0,11):\n",
    "    token_at_t = np.argmax(outs[0][0][t])\n",
    "    acum+=reverse_target_char_index[token_at_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '0', '0', '4', '-', '0', '3', '-', '2', '2', '<', 'e', 'n', 'd', '>']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2004-03-22<end>'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(encoder_input_data[2:3], np.where(oh_start_char==1)[0][0],encoder_model, decoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, index_start_char,encoder_model, decoder_model):\n",
    "    # Encode the input sequence as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, index_start_char] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    decoded_sentence = ''\n",
    "    for char_elem in range(11) :\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "  \n",
    "        # Sample a token wih argmax\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        # Get char associatedwith token\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states. Here we reassign the initial decoder states.  \n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input date: tuesday march 28 1995<pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Predicted date 1995-03-28<end>\n",
      "Expected result is 1995-03-28<end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input date:\", from_oh_to_chars(encoder_input_data[0], reverse_input_char_index))\n",
    "print(\"Predicted date\", decode_sequence(encoder_input_data[0:1], np.where(oh_start_char==1)[0][0], encoder_model, decoder_model)) # '1990-04-28<end>'\n",
    "print(\"Expected result is\", from_oh_to_chars(decoder_target_data[0], reverse_target_char_index)) # '1990-04-28<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input date: 22 march 2004<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Predicted date 2004-03-22<end>\n",
      "Expected result is 2004-03-22<end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input date:\", from_oh_to_chars(encoder_input_data[2], reverse_input_char_index))\n",
    "print(\"Predicted date\", decode_sequence(encoder_input_data[2:3], np.where(oh_start_char==1)[0][0], encoder_model, decoder_model)) # '1990-04-28<end>'\n",
    "print(\"Expected result is\", from_oh_to_chars(decoder_target_data[2], reverse_target_char_index)) # '1990-04-28<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['matches','decoded','expected', 'input_sentence'])\n",
    "df.loc[0, ['matches','decoded', 'expected', 'input_sentence']]=[True, 'decoded_value','true_value', 'elem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, elem in enumerate(encoder_input_data[0:10000]):\n",
    "    input_seq = elem.reshape(1,30,37)\n",
    "    decoded_value = decode_sequence(input_seq,np.where(oh_start_char==1)[0][0],encoder_model, decoder_model)\n",
    "    true_value = from_oh_to_chars(decoder_target_data[i], reverse_target_char_index)\n",
    "    matches = decoded_value==true_value\n",
    "    df.loc[i, ['matches','decoded', 'expected', 'input_sentence']]=[matches, decoded_value,true_value,  from_oh_to_chars(elem, reverse_input_char_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     9826\n",
       "False     174\n",
       "Name: matches, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.matches.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['years'] = df.expected.str[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with data in the triaining set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: tuesday march 28 1995\n",
      "Decoded sentence: 1995-03-28<end> (should be  1995-03-28 )\n",
      "Input sentence: 27 may 2014\n",
      "Decoded sentence: 2014-05-27<end> (should be  2014-05-27 )\n",
      "Input sentence: 22 march 2004\n",
      "Decoded sentence: 2004-03-22<end> (should be  2004-03-22 )\n",
      "Input sentence: saturday october 29 1988\n",
      "Decoded sentence: 1988-10-29<end> (should be  1988-10-29 )\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(4):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq,np.where(oh_start_char==1)[0][0],encoder_model, decoder_model)\n",
    "\n",
    "    print('Input sentence:', dataset[seq_index][0])\n",
    "    print('Decoded sentence:', decoded_sentence , '(should be ', dataset[seq_index][1], ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thursday january 26 1995<pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_oh_to_chars(input_seq[0], reverse_input_char_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 20093.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#Load dates datasets\n",
    "m = 10000\n",
    "dataset_test, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CHECK]\n",
    "Tx = 30\n",
    "Ty = 10\n",
    "X_test, Y_test, Xoh_test, Yoh_test = preprocess_data(dataset_test, human_vocab, machine_vocab, Tx, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998-06-20\n",
      "jun 20 1998<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "i=np.random.randint(0,10000)\n",
    "print(from_encode_to_chars(Y_test[i], reverse_target_char_index))\n",
    "print(from_encode_to_chars(X_test[i], reverse_input_char_index))\n",
    "#print(from_oh_to_chars(decoder_input_data[0], reverse_target_char_index)) # '<start>1998-05-09'\n",
    "#print(from_oh_to_chars(decoder_target_data[0], reverse_target_char_index)) # '1998-05-09<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this dataset to check whether the test date is already in the input dataset\n",
    "nl_input_data= [from_encode_to_chars(X_test[i], reverse_input_char_index).replace('<pad>','') for i in range(0,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_date_to_oh(date_nl, nl_input_data):\n",
    "    # look if date in argument is already present in training set\n",
    "    if date_nl.replace('<pad>','') in nl_input_data:\n",
    "        raise ValueError('date is already in input dataset ')\n",
    "    \n",
    "    oh_new_phrase = from_nl_to_oh(date_nl, human_vocab, reverse_input_char_index, 30)\n",
    "    oh_new_date = np.zeros((1,oh_new_phrase.shape[0], num_encoder_tokens))\n",
    "    oh_new_date[0] = oh_new_phrase\n",
    "    return oh_new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sunday july 23 1995',\n",
       " 'wednesday june 5 1996',\n",
       " '19 feb 2011',\n",
       " 'monday october 29 1979',\n",
       " '20 june 2005',\n",
       " '21 10 09',\n",
       " '11 09 83',\n",
       " '20 november 2004',\n",
       " '16 january 1997',\n",
       " '29 nov 1971',\n",
       " '26.10.72',\n",
       " 'saturday may 12 1984',\n",
       " 'thursday september 2 1971',\n",
       " 'sunday november 13 2005',\n",
       " 'saturday march 22 2008',\n",
       " 'june 27 2012',\n",
       " 'friday october 7 2016',\n",
       " 'thursday february 19 2009',\n",
       " '5 nov 2013',\n",
       " '8 jun 1988',\n",
       " 'february 22 2009',\n",
       " 'feb 11 2010',\n",
       " 'sunday september 2 2018',\n",
       " 'sunday december 13 1992',\n",
       " '14 sep 2016',\n",
       " '03.02.77',\n",
       " '1 september 1994',\n",
       " 'december 26 2011',\n",
       " '21 aug 2001',\n",
       " '8/26/88',\n",
       " 'monday september 27 2004',\n",
       " '03.01.92',\n",
       " 'march 5 1975',\n",
       " '4 08 85',\n",
       " 'saturday november 28 1987',\n",
       " 'wednesday october 18 2017',\n",
       " '11/9/15',\n",
       " 'saturday january 29 1977',\n",
       " 'friday june 26 1970',\n",
       " '21 august 1983',\n",
       " 'thursday october 27 2016',\n",
       " '26 sep 1984',\n",
       " '24.06.08',\n",
       " '18 aug 2008',\n",
       " 'november 20 2010',\n",
       " 'friday september 25 1992',\n",
       " 'jul 10 1985',\n",
       " '12/23/91',\n",
       " 'friday july 7 1995',\n",
       " '24 aug 1997',\n",
       " '1 august 2013',\n",
       " 'thursday january 7 1993',\n",
       " 'saturday february 22 2014',\n",
       " 'sunday july 29 2007',\n",
       " 'saturday october 9 2004',\n",
       " 'saturday september 8 2001',\n",
       " 'january 17 1999',\n",
       " 'friday january 15 2010',\n",
       " 'saturday january 3 2015',\n",
       " 'monday july 28 2014',\n",
       " '22 aug 2011',\n",
       " '30.01.83',\n",
       " 'tuesday july 20 1993',\n",
       " 'monday may 25 1998',\n",
       " 'sunday august 17 2008',\n",
       " 'august 18 2000',\n",
       " 'thursday december 26 1991',\n",
       " 'friday june 24 1994',\n",
       " 'sunday november 16 1986',\n",
       " '2 feb 2015',\n",
       " 'may 23 2000',\n",
       " '09 sep 1997',\n",
       " 'sunday august 7 2011',\n",
       " 'thursday march 22 1990',\n",
       " '12/28/00',\n",
       " '21 apr 2004',\n",
       " 'thursday august 2 2018',\n",
       " 'august 17 2012',\n",
       " 'oct 8 1986',\n",
       " '10 september 1975',\n",
       " 'wednesday june 9 1976',\n",
       " '24 jan 1970',\n",
       " 'wednesday september 13 1978',\n",
       " '11/10/75',\n",
       " '4 mar 2009',\n",
       " 'november 19 1974',\n",
       " '3 05 11',\n",
       " 'wednesday october 15 1980',\n",
       " 'may 20 1997',\n",
       " 'july 13 2016',\n",
       " '12/26/87',\n",
       " '22 jun 1970',\n",
       " 'sunday april 27 1997',\n",
       " 'friday january 16 1998',\n",
       " 'september 26 1994',\n",
       " '28 may 1980',\n",
       " 'sunday october 12 2003',\n",
       " '5 february 2003',\n",
       " 'monday october 21 1974',\n",
       " '03.08.82',\n",
       " '28 08 77',\n",
       " 'saturday april 2 2005',\n",
       " '21 nov 1984',\n",
       " '7 05 02',\n",
       " '9/6/05',\n",
       " '21 09 72',\n",
       " '11/15/75',\n",
       " '05 nov 1977',\n",
       " '1 12 96',\n",
       " '24 may 1991',\n",
       " '20 october 1998',\n",
       " '15 mar 2017',\n",
       " 'august 6 1976',\n",
       " 'wednesday may 29 1991',\n",
       " '9 apr 2002',\n",
       " 'october 1 1972',\n",
       " 'wednesday june 5 2013',\n",
       " '2/10/94',\n",
       " 'may 5 1984',\n",
       " 'sep 23 1975',\n",
       " 'tuesday october 15 1996',\n",
       " 'thursday november 6 2003',\n",
       " '5 february 1990',\n",
       " '1 jan 1974',\n",
       " 'sunday april 29 2001',\n",
       " 'thursday june 25 1992',\n",
       " '19.02.72',\n",
       " '29 may 1980',\n",
       " 'monday february 21 2000',\n",
       " '12 sep 1990',\n",
       " 'sunday may 21 2006',\n",
       " 'friday may 16 1986',\n",
       " 'wednesday june 18 2003',\n",
       " '27 nov 1985',\n",
       " 'sunday march 25 2018',\n",
       " 'october 29 1982',\n",
       " '25 oct 1978',\n",
       " '25 apr 2004',\n",
       " '6/20/11',\n",
       " 'monday february 3 1986',\n",
       " 'friday april 17 2015',\n",
       " 'wednesday july 20 2011',\n",
       " '25 february 2009',\n",
       " '8 jan 1992',\n",
       " 'wednesday february 24 2010',\n",
       " '10 may 1980',\n",
       " 'sunday october 26 2008',\n",
       " 'friday december 28 1979',\n",
       " '9 november 2017',\n",
       " 'saturday march 8 1975',\n",
       " 'friday may 8 1981',\n",
       " '21 06 98',\n",
       " 'august 21 1991',\n",
       " 'tuesday january 5 1971',\n",
       " 'friday november 23 2012',\n",
       " 'july 8 2006',\n",
       " '13 apr 2015',\n",
       " 'sunday october 22 1972',\n",
       " 'friday july 17 1992',\n",
       " 'friday april 19 2002',\n",
       " '19 august 1991',\n",
       " 'friday may 4 1973',\n",
       " 'saturday june 11 1977',\n",
       " '19 january 2002',\n",
       " '2/6/75',\n",
       " '25 july 1980',\n",
       " 'tuesday november 15 1983',\n",
       " '25 jun 1970',\n",
       " '24 sep 2007',\n",
       " 'wednesday september 23 2015',\n",
       " '21 may 2016',\n",
       " 'december 25 1993',\n",
       " '21 july 2005',\n",
       " '21 may 1971',\n",
       " '11 jul 2008',\n",
       " '8 08 81',\n",
       " 'tuesday august 21 1973',\n",
       " '2 may 1974',\n",
       " 'thursday august 2 2001',\n",
       " '26 jun 2006',\n",
       " '23.06.88',\n",
       " 'monday november 7 2005',\n",
       " 'tuesday january 11 2011',\n",
       " 'sunday may 30 1976',\n",
       " '6 aug 2016',\n",
       " '04 sep 2003',\n",
       " 'wednesday march 13 1996',\n",
       " '28 september 1982',\n",
       " 'tuesday december 18 1984',\n",
       " 'may 2 2000',\n",
       " 'sunday august 28 1977',\n",
       " '26.07.09',\n",
       " 'wednesday december 11 1985',\n",
       " '21 july 2011',\n",
       " '20 jan 1987',\n",
       " 'august 15 1983',\n",
       " 'january 21 1989',\n",
       " 'december 17 1970',\n",
       " '3 july 2001',\n",
       " '19 january 1986',\n",
       " '14 november 1993',\n",
       " 'thursday may 9 1996',\n",
       " '15 12 95',\n",
       " '20 aug 2010',\n",
       " 'september 1 2000',\n",
       " '1/22/99',\n",
       " 'monday june 2 1975',\n",
       " 'tuesday october 4 1983',\n",
       " '31 may 2007',\n",
       " 'july 7 1976',\n",
       " '13 august 1986',\n",
       " '26.06.93',\n",
       " '20 july 1970',\n",
       " '27 jun 1979',\n",
       " 'thursday august 9 2012',\n",
       " '26 may 1975',\n",
       " '11 march 1976',\n",
       " 'november 7 1984',\n",
       " 'friday june 4 1971',\n",
       " 'sunday august 9 1992',\n",
       " '22 08 71',\n",
       " 'sep 18 2012',\n",
       " '07 jul 1992',\n",
       " 'saturday october 14 2000',\n",
       " '18 jan 2000',\n",
       " '30 may 2009',\n",
       " 'february 20 2016',\n",
       " 'friday june 12 1981',\n",
       " 'sunday december 5 1999',\n",
       " 'sunday december 9 1990',\n",
       " 'jan 9 1998',\n",
       " '4 july 1973',\n",
       " 'march 31 2001',\n",
       " 'wednesday september 25 2002',\n",
       " '19 september 2005',\n",
       " 'sep 4 2012',\n",
       " 'monday august 18 1975',\n",
       " 'tuesday february 2 1988',\n",
       " '4 october 2012',\n",
       " 'jun 25 1999',\n",
       " 'january 31 2011',\n",
       " '18 dec 2002',\n",
       " '9 dec 2015',\n",
       " '7 06 00',\n",
       " 'tuesday february 6 1996',\n",
       " '20 february 2005',\n",
       " '16 09 89',\n",
       " 'sunday september 27 1987',\n",
       " '22 08 76',\n",
       " 'january 25 1999',\n",
       " '6 aug 1990',\n",
       " '15 august 1993',\n",
       " '14.06.83',\n",
       " 'tuesday january 15 2008',\n",
       " '29 01 03',\n",
       " 'august 7 2010',\n",
       " 'july 25 2014',\n",
       " 'thursday april 4 2002',\n",
       " '20 december 2005',\n",
       " '25 05 00',\n",
       " 'monday march 9 1987',\n",
       " 'thursday april 19 2007',\n",
       " 'tuesday march 14 1995',\n",
       " 'saturday january 9 1993',\n",
       " 'friday march 2 2001',\n",
       " 'thursday september 2 1982',\n",
       " 'tuesday march 2 2004',\n",
       " '21 june 1988',\n",
       " '20 may 1972',\n",
       " '21 dec 1999',\n",
       " 'monday may 14 1990',\n",
       " 'wednesday september 6 2000',\n",
       " 'wednesday march 1 2006',\n",
       " '26 jan 2017',\n",
       " 'october 6 2015',\n",
       " '10 dec 2009',\n",
       " 'tuesday april 26 2011',\n",
       " 'monday june 9 2003',\n",
       " '22 october 1999',\n",
       " 'wednesday may 29 1996',\n",
       " 'friday june 28 1974',\n",
       " 'tuesday october 23 2007',\n",
       " '21 september 1996',\n",
       " 'saturday june 4 1988',\n",
       " '11 jun 2016',\n",
       " 'monday august 15 2005',\n",
       " 'mar 25 1982',\n",
       " 'monday december 13 2004',\n",
       " '10 may 2000',\n",
       " '13.08.08',\n",
       " 'thursday may 11 1995',\n",
       " '6 02 97',\n",
       " '6 apr 2011',\n",
       " 'sunday october 27 1991',\n",
       " '08 jul 2016',\n",
       " 'december 28 1987',\n",
       " 'sunday march 22 1992',\n",
       " '07 oct 1990',\n",
       " 'november 7 1981',\n",
       " '5 june 2000',\n",
       " '9 march 2001',\n",
       " '12 jul 2005',\n",
       " 'may 16 1973',\n",
       " '2/6/86',\n",
       " '02 sep 1984',\n",
       " 'wednesday june 8 2011',\n",
       " '06 jul 1991',\n",
       " 'saturday may 14 1977',\n",
       " '17 feb 1989',\n",
       " '16 july 1974',\n",
       " '15 january 1979',\n",
       " 'friday july 13 1973',\n",
       " '28 aug 2008',\n",
       " 'friday august 3 1973',\n",
       " '28 05 02',\n",
       " 'saturday february 28 2004',\n",
       " 'wednesday march 21 1990',\n",
       " '02.09.16',\n",
       " '22 09 87',\n",
       " 'saturday december 5 1981',\n",
       " 'monday august 2 1971',\n",
       " 'monday august 19 2013',\n",
       " 'may 16 1974',\n",
       " '20 jun 1982',\n",
       " '21 jun 1980',\n",
       " 'monday november 29 1971',\n",
       " 'saturday october 6 1973',\n",
       " 'wednesday september 25 1991',\n",
       " '24 july 2013',\n",
       " '4 september 2012',\n",
       " 'wednesday april 4 1973',\n",
       " 'friday june 3 2016',\n",
       " 'tuesday february 9 1982',\n",
       " 'october 22 2008',\n",
       " 'sunday july 10 1994',\n",
       " 'wednesday september 21 1977',\n",
       " '2 07 75',\n",
       " 'wednesday september 28 1977',\n",
       " '10/13/74',\n",
       " 'june 20 1993',\n",
       " 'tuesday august 14 2007',\n",
       " 'april 2 1997',\n",
       " '6/10/15',\n",
       " 'wednesday april 14 2004',\n",
       " 'thursday december 3 1981',\n",
       " 'friday december 16 1994',\n",
       " 'apr 5 1996',\n",
       " 'february 5 1986',\n",
       " 'sunday august 18 1991',\n",
       " '23 november 2004',\n",
       " '7/4/05',\n",
       " 'saturday june 12 2004',\n",
       " '29 10 90',\n",
       " 'june 6 2006',\n",
       " '20 03 76',\n",
       " 'monday december 20 1993',\n",
       " 'friday december 19 1975',\n",
       " '25 mar 1993',\n",
       " 'may 2 1999',\n",
       " '10 06 88',\n",
       " 'monday march 22 1993',\n",
       " 'tuesday september 17 1985',\n",
       " 'dec 28 2004',\n",
       " '30.10.97',\n",
       " '17 jun 2008',\n",
       " 'may 1 2012',\n",
       " 'november 16 1997',\n",
       " 'dec 24 1988',\n",
       " 'february 10 1976',\n",
       " 'thursday december 9 1999',\n",
       " 'april 1 1997',\n",
       " 'wednesday february 18 2015',\n",
       " '30.07.75',\n",
       " '6 july 2004',\n",
       " 'august 29 2010',\n",
       " 'september 16 2012',\n",
       " '1 june 1988',\n",
       " 'tuesday august 7 1979',\n",
       " 'december 7 1975',\n",
       " 'saturday april 1 2017',\n",
       " '03 oct 1973',\n",
       " 'thursday january 2 1975',\n",
       " '15 sep 1979',\n",
       " '3/24/18',\n",
       " 'aug 7 1979',\n",
       " '18 nov 1993',\n",
       " 'july 14 2017',\n",
       " 'monday august 10 1992',\n",
       " '2 march 1991',\n",
       " 'october 29 2003',\n",
       " '14 jul 1982',\n",
       " '12/24/92',\n",
       " '19 dec 1998',\n",
       " '22 dec 2001',\n",
       " 'june 4 1980',\n",
       " 'january 13 2003',\n",
       " '29 04 72',\n",
       " 'october 10 1976',\n",
       " 'saturday may 2 1987',\n",
       " '20 feb 1991',\n",
       " 'july 24 2003',\n",
       " 'june 12 2004',\n",
       " '13 mar 2010',\n",
       " '30 january 2004',\n",
       " 'sunday february 12 2017',\n",
       " 'thursday december 18 1997',\n",
       " 'monday july 4 1988',\n",
       " 'jan 17 2001',\n",
       " '30 jun 1981',\n",
       " 'sunday march 21 1971',\n",
       " '8/22/01',\n",
       " '06.04.14',\n",
       " '10 feb 2001',\n",
       " 'sunday december 1 1985',\n",
       " '8 june 2009',\n",
       " '25 sep 1971',\n",
       " 'friday september 26 2014',\n",
       " 'august 9 1975',\n",
       " '2 03 70',\n",
       " 'sunday march 27 2011',\n",
       " 'sep 18 2004',\n",
       " '29.05.06',\n",
       " 'sunday july 23 1978',\n",
       " 'friday august 5 2005',\n",
       " 'saturday may 16 1987',\n",
       " '02.03.81',\n",
       " 'wednesday october 18 1972',\n",
       " 'tuesday february 20 2007',\n",
       " '31 oct 2002',\n",
       " 'tuesday july 20 2004',\n",
       " '11 oct 1984',\n",
       " 'jun 11 1996',\n",
       " 'june 22 1972',\n",
       " 'friday october 5 2012',\n",
       " 'thursday january 5 1978',\n",
       " 'sunday october 20 1974',\n",
       " 'oct 23 1992',\n",
       " 'june 1 1988',\n",
       " '19 aug 1988',\n",
       " '20 feb 1993',\n",
       " 'sep 8 2003',\n",
       " '18 march 1988',\n",
       " '5/30/81',\n",
       " 'august 31 2005',\n",
       " '1 november 1973',\n",
       " 'saturday november 25 2017',\n",
       " '16 mar 1990',\n",
       " '12 apr 1981',\n",
       " 'thursday august 8 2013',\n",
       " 'sunday august 17 2008',\n",
       " '12/12/81',\n",
       " '10/6/03',\n",
       " '17 jan 2018',\n",
       " 'wednesday february 19 1975',\n",
       " 'saturday september 2 1995',\n",
       " 'saturday december 31 1977',\n",
       " 'friday october 15 1976',\n",
       " 'jun 28 1971',\n",
       " '13 july 1993',\n",
       " 'monday august 20 1984',\n",
       " 'sunday september 7 1975',\n",
       " 'jul 21 1995',\n",
       " 'saturday january 1 2005',\n",
       " 'saturday october 21 1989',\n",
       " 'june 4 1998',\n",
       " '12/25/14',\n",
       " '14 may 1989',\n",
       " 'saturday january 10 1981',\n",
       " '1/24/89',\n",
       " '13 feb 1995',\n",
       " 'thursday december 12 2002',\n",
       " '15 07 70',\n",
       " 'october 21 2016',\n",
       " '15 jul 2017',\n",
       " 'sunday february 8 2004',\n",
       " 'wednesday september 19 1984',\n",
       " '31 july 1996',\n",
       " '5 august 1985',\n",
       " 'september 12 1992',\n",
       " '18 may 1978',\n",
       " 'tuesday june 5 1990',\n",
       " 'thursday february 17 2005',\n",
       " 'friday february 6 1981',\n",
       " '2/20/85',\n",
       " 'monday november 18 2002',\n",
       " '18 december 2005',\n",
       " 'sunday november 25 2007',\n",
       " '5 jan 1974',\n",
       " '09 jan 2010',\n",
       " 'tuesday march 8 2016',\n",
       " 'sunday august 5 2018',\n",
       " 'sunday july 5 1998',\n",
       " 'wednesday september 14 1988',\n",
       " 'tuesday june 23 1998',\n",
       " 'tuesday june 20 2000',\n",
       " 'monday july 22 1991',\n",
       " 'july 22 2011',\n",
       " 'june 29 1986',\n",
       " 'jul 1 2003',\n",
       " '29 oct 2007',\n",
       " 'monday july 3 1989',\n",
       " '02 mar 1987',\n",
       " 'tuesday may 9 1972',\n",
       " 'tuesday october 26 1976',\n",
       " '3 march 1995',\n",
       " 'friday april 24 2009',\n",
       " 'tuesday august 15 1978',\n",
       " '15 january 2005',\n",
       " 'saturday september 17 2016',\n",
       " '17 oct 1998',\n",
       " '21 oct 1978',\n",
       " '23 december 1985',\n",
       " 'wednesday april 25 1979',\n",
       " 'may 27 1983',\n",
       " 'may 19 1986',\n",
       " '17.11.79',\n",
       " 'saturday november 27 1976',\n",
       " 'tuesday september 5 2006',\n",
       " '14 february 1985',\n",
       " '27 jul 2014',\n",
       " '21.09.83',\n",
       " '20 august 1995',\n",
       " '17 february 1992',\n",
       " '08 aug 1973',\n",
       " '7/11/93',\n",
       " 'nov 19 1973',\n",
       " '8/20/82',\n",
       " '1 02 00',\n",
       " '04.01.80',\n",
       " 'monday december 13 1971',\n",
       " '16 september 2006',\n",
       " 'monday june 30 1997',\n",
       " '18 january 2016',\n",
       " '25 jun 1997',\n",
       " '30 oct 1987',\n",
       " '17 august 1973',\n",
       " 'friday october 4 1991',\n",
       " 'april 24 1980',\n",
       " 'april 18 2000',\n",
       " 'sunday october 8 2017',\n",
       " 'saturday april 14 2018',\n",
       " '14 february 2018',\n",
       " '6 apr 2017',\n",
       " 'may 31 2003',\n",
       " 'friday september 11 1987',\n",
       " '21 08 12',\n",
       " 'thursday december 28 2000',\n",
       " 'september 30 1993',\n",
       " 'oct 19 2001',\n",
       " 'friday may 4 2018',\n",
       " '10 mar 1993',\n",
       " '3 01 82',\n",
       " '19 jan 1995',\n",
       " '10 october 1992',\n",
       " '2 apr 1976',\n",
       " 'mar 31 2018',\n",
       " '22 november 1974',\n",
       " '16 jun 1980',\n",
       " 'sunday july 9 1972',\n",
       " 'friday march 27 1992',\n",
       " 'wednesday january 17 1996',\n",
       " 'friday may 15 1992',\n",
       " 'monday october 7 1974',\n",
       " 'january 27 2012',\n",
       " '1/15/12',\n",
       " 'friday september 5 1975',\n",
       " '5 11 86',\n",
       " 'friday june 11 2010',\n",
       " 'monday may 7 2018',\n",
       " '3 jul 1973',\n",
       " 'june 17 1987',\n",
       " 'thursday january 6 1994',\n",
       " 'jan 14 2005',\n",
       " '6/3/81',\n",
       " '9 january 1988',\n",
       " '05 jan 1976',\n",
       " '05.01.93',\n",
       " 'thursday november 6 1986',\n",
       " '26.03.98',\n",
       " '23 november 1981',\n",
       " 'wednesday november 4 2015',\n",
       " 'wednesday october 11 1989',\n",
       " '24 sep 1985',\n",
       " 'saturday february 2 2002',\n",
       " 'thursday july 2 1992',\n",
       " '4/13/70',\n",
       " '06 jul 1976',\n",
       " 'saturday september 24 1983',\n",
       " 'tuesday october 29 1991',\n",
       " 'wednesday july 26 1978',\n",
       " '19.04.93',\n",
       " '19 march 1999',\n",
       " 'august 18 1994',\n",
       " 'friday october 19 1979',\n",
       " 'friday june 19 1987',\n",
       " 'june 16 1988',\n",
       " '4 06 88',\n",
       " '31 may 2001',\n",
       " '23 jun 2016',\n",
       " 'tuesday january 10 2006',\n",
       " 'monday august 25 2003',\n",
       " '6 september 1977',\n",
       " 'apr 19 1983',\n",
       " '25.07.12',\n",
       " 'sunday march 14 1999',\n",
       " 'tuesday march 19 2013',\n",
       " '8 jun 2002',\n",
       " 'thursday january 26 2017',\n",
       " 'sunday april 4 2004',\n",
       " 'friday july 2 1993',\n",
       " '1/21/78',\n",
       " '2 july 2014',\n",
       " 'tuesday december 22 1987',\n",
       " '16 aug 1974',\n",
       " 'saturday december 12 1987',\n",
       " '30 apr 2002',\n",
       " '4 oct 1979',\n",
       " 'wednesday june 23 1976',\n",
       " 'thursday december 7 1995',\n",
       " 'dec 19 1980',\n",
       " 'january 24 1985',\n",
       " 'wednesday may 15 1985',\n",
       " '14 february 1993',\n",
       " 'nov 6 1972',\n",
       " 'wednesday september 25 1991',\n",
       " '8 february 1976',\n",
       " 'september 20 1972',\n",
       " 'wednesday march 24 1993',\n",
       " 'december 24 2014',\n",
       " '10 jan 1991',\n",
       " 'sunday september 3 2000',\n",
       " 'wednesday march 28 1973',\n",
       " 'march 15 1999',\n",
       " 'saturday april 6 1985',\n",
       " '1 dec 1996',\n",
       " '3/17/96',\n",
       " '17 may 1994',\n",
       " '24 jun 2011',\n",
       " 'saturday june 1 1996',\n",
       " 'tuesday september 17 2002',\n",
       " 'monday february 16 2015',\n",
       " 'april 14 2007',\n",
       " 'sunday june 15 2008',\n",
       " 'saturday march 21 1981',\n",
       " '20 september 1981',\n",
       " 'march 29 2008',\n",
       " '6 december 1981',\n",
       " '18.02.70',\n",
       " '6 august 1998',\n",
       " '3 sep 1995',\n",
       " 'saturday april 8 2000',\n",
       " 'thursday january 16 1975',\n",
       " '7/22/78',\n",
       " '17.05.09',\n",
       " 'june 17 2002',\n",
       " '24 sep 2012',\n",
       " '11 april 2002',\n",
       " '16 april 1986',\n",
       " 'sunday april 1 2007',\n",
       " 'saturday january 31 1987',\n",
       " 'tuesday june 13 1978',\n",
       " 'saturday april 7 2007',\n",
       " 'october 29 2009',\n",
       " '23 august 1990',\n",
       " 'monday april 21 2008',\n",
       " 'wednesday january 22 1992',\n",
       " 'wednesday february 13 2013',\n",
       " '29.06.01',\n",
       " '21.08.81',\n",
       " 'august 31 2014',\n",
       " '15 jan 1976',\n",
       " 'saturday august 31 2002',\n",
       " '15 mar 2006',\n",
       " '4 april 1978',\n",
       " 'friday august 11 1995',\n",
       " 'april 28 2006',\n",
       " 'monday may 14 2018',\n",
       " '8 february 1989',\n",
       " 'august 15 1986',\n",
       " 'sunday april 15 2012',\n",
       " '22 november 1970',\n",
       " '21 jan 2001',\n",
       " 'thursday may 14 1981',\n",
       " 'may 28 1999',\n",
       " 'wednesday april 13 2005',\n",
       " '30.10.87',\n",
       " '07.01.83',\n",
       " '30 apr 2015',\n",
       " 'friday june 15 1973',\n",
       " '15 02 03',\n",
       " 'september 29 1991',\n",
       " '3/9/79',\n",
       " '24 may 1989',\n",
       " '9 october 2010',\n",
       " 'december 29 2011',\n",
       " '26 01 74',\n",
       " '7 jun 2007',\n",
       " '21 september 1999',\n",
       " '08 jan 1973',\n",
       " 'friday january 17 1992',\n",
       " 'thursday april 9 2009',\n",
       " 'thursday december 23 2010',\n",
       " '28 apr 2013',\n",
       " '31 05 98',\n",
       " '01 may 1973',\n",
       " 'tuesday march 11 1975',\n",
       " 'april 15 2002',\n",
       " 'friday july 7 1989',\n",
       " 'friday june 23 2006',\n",
       " 'wednesday february 25 2009',\n",
       " 'tuesday may 20 2008',\n",
       " 'july 16 2004',\n",
       " 'sunday december 6 2009',\n",
       " 'monday january 10 1994',\n",
       " '2 april 2006',\n",
       " '5/31/14',\n",
       " 'thursday march 15 1979',\n",
       " 'friday july 13 2007',\n",
       " 'friday september 28 2012',\n",
       " '15 april 2017',\n",
       " 'january 9 1973',\n",
       " 'sunday october 19 1986',\n",
       " '21 june 2005',\n",
       " 'sunday february 20 1977',\n",
       " 'saturday february 13 1993',\n",
       " 'august 29 2002',\n",
       " '6 10 98',\n",
       " 'thursday december 9 2010',\n",
       " '6 11 07',\n",
       " 'saturday september 2 1989',\n",
       " 'april 20 2018',\n",
       " 'sunday january 18 2004',\n",
       " '01 jul 1995',\n",
       " 'february 18 1977',\n",
       " '14 january 1979',\n",
       " '4 jan 2018',\n",
       " '22 10 79',\n",
       " '16 05 07',\n",
       " '24.05.82',\n",
       " '14.10.94',\n",
       " 'october 7 2007',\n",
       " 'friday april 29 2011',\n",
       " 'may 23 1993',\n",
       " '26 jun 1998',\n",
       " 'friday february 18 1983',\n",
       " 'monday march 24 1997',\n",
       " 'september 22 1977',\n",
       " 'monday january 13 1997',\n",
       " 'sunday january 2 1972',\n",
       " 'sep 22 1974',\n",
       " 'february 17 2011',\n",
       " '07 may 1983',\n",
       " 'wednesday june 30 2004',\n",
       " '09.10.08',\n",
       " 'tuesday january 15 2013',\n",
       " '04.06.11',\n",
       " 'saturday august 2 2003',\n",
       " '3 dec 1996',\n",
       " 'april 4 1995',\n",
       " 'saturday october 29 2016',\n",
       " 'thursday march 27 1975',\n",
       " 'monday october 22 1984',\n",
       " '15 jul 1981',\n",
       " '27 sep 1976',\n",
       " 'august 10 2018',\n",
       " 'monday june 10 1985',\n",
       " 'thursday january 24 1980',\n",
       " '12 dec 1972',\n",
       " '27 jul 1991',\n",
       " 'may 27 2010',\n",
       " 'friday may 11 2007',\n",
       " 'september 8 1993',\n",
       " 'thursday february 22 1979',\n",
       " 'july 30 2013',\n",
       " '27 mar 1982',\n",
       " 'friday february 2 2001',\n",
       " '29 jun 2003',\n",
       " 'jan 27 2006',\n",
       " '16 jun 2010',\n",
       " '23 april 2013',\n",
       " 'tuesday june 17 2003',\n",
       " 'april 22 1976',\n",
       " 'thursday november 12 1992',\n",
       " '8/21/91',\n",
       " 'monday september 18 2017',\n",
       " 'tuesday october 22 1996',\n",
       " 'monday august 2 1971',\n",
       " 'friday march 17 2006',\n",
       " '26 mar 2006',\n",
       " 'thursday april 13 1989',\n",
       " '6/30/79',\n",
       " '11 aug 1983',\n",
       " '03.04.71',\n",
       " '1 march 1977',\n",
       " '30.07.00',\n",
       " '30 04 12',\n",
       " '11 february 1976',\n",
       " '22 july 2017',\n",
       " '03.02.91',\n",
       " '14 september 2005',\n",
       " 'monday september 21 1992',\n",
       " 'saturday january 9 1993',\n",
       " 'thursday february 1 2001',\n",
       " '29 apr 1999',\n",
       " '29 jan 1972',\n",
       " 'friday june 17 2005',\n",
       " '17 jan 1998',\n",
       " 'friday july 3 1987',\n",
       " '23 oct 1983',\n",
       " 'may 7 1984',\n",
       " '12 january 1996',\n",
       " 'saturday december 26 1981',\n",
       " 'sunday january 12 1975',\n",
       " '12 may 2008',\n",
       " 'thursday march 9 1989',\n",
       " '8 12 77',\n",
       " 'friday may 27 1983',\n",
       " 'august 1 2001',\n",
       " 'saturday september 7 1974',\n",
       " 'december 12 1996',\n",
       " '26.10.02',\n",
       " 'thursday september 5 2002',\n",
       " '10/24/14',\n",
       " 'wednesday august 21 1974',\n",
       " 'monday october 24 1983',\n",
       " 'friday july 17 1992',\n",
       " 'saturday may 9 1970',\n",
       " 'wednesday june 21 2006',\n",
       " '27 mar 1983',\n",
       " 'friday march 30 1984',\n",
       " '06 mar 1976',\n",
       " 'wednesday september 5 2007',\n",
       " 'aug 11 1996',\n",
       " '10 april 1993',\n",
       " '17 jun 1974',\n",
       " 'tuesday november 28 1978',\n",
       " '27 03 06',\n",
       " 'saturday july 19 1986',\n",
       " '14 may 2008',\n",
       " '14 sep 1998',\n",
       " 'thursday january 20 1994',\n",
       " '11 november 1992',\n",
       " 'september 29 1970',\n",
       " 'sunday february 16 2003',\n",
       " '26 january 1974',\n",
       " 'tuesday october 6 2015',\n",
       " 'february 5 2018',\n",
       " 'july 15 1998',\n",
       " '25 june 1990',\n",
       " 'sunday may 29 1977',\n",
       " '3 february 1981',\n",
       " 'saturday march 15 2003',\n",
       " 'april 26 2002',\n",
       " 'thursday january 10 1985',\n",
       " '21 december 1978',\n",
       " 'march 24 1980',\n",
       " '8 dec 2013',\n",
       " 'tuesday september 26 1995',\n",
       " 'friday june 8 1990',\n",
       " '1 03 96',\n",
       " '06.07.77',\n",
       " 'wednesday october 14 2015',\n",
       " '26.04.82',\n",
       " 'wednesday april 7 1971',\n",
       " 'friday november 25 1977',\n",
       " 'july 4 2008',\n",
       " '26 sep 2002',\n",
       " 'tuesday june 21 1988',\n",
       " 'february 18 1980',\n",
       " '11/30/16',\n",
       " '15 10 02',\n",
       " 'dec 30 1985',\n",
       " '9/4/09',\n",
       " 'thursday september 6 1979',\n",
       " 'wednesday april 10 1985',\n",
       " 'wednesday october 18 2017',\n",
       " '15 12 15',\n",
       " '1 04 17',\n",
       " '22 oct 1999',\n",
       " 'monday july 1 1996',\n",
       " 'tuesday august 28 1973',\n",
       " 'friday april 4 1986',\n",
       " 'tuesday may 23 2000',\n",
       " 'thursday february 15 1979',\n",
       " 'thursday may 14 1992',\n",
       " '15 august 1993',\n",
       " '25 jun 2000',\n",
       " 'monday march 31 1975',\n",
       " 'thursday march 16 1972',\n",
       " '20 june 2004',\n",
       " 'friday march 21 1997',\n",
       " 'friday december 8 1995',\n",
       " 'december 30 2002',\n",
       " '2 jun 1984',\n",
       " 'saturday june 8 1985',\n",
       " 'september 6 1979',\n",
       " '09 apr 1993',\n",
       " 'wednesday october 14 2015',\n",
       " 'saturday january 18 1975',\n",
       " 'saturday december 4 2010',\n",
       " 'january 16 1979',\n",
       " '22.04.77',\n",
       " 'sep 30 1996',\n",
       " 'wednesday august 6 2008',\n",
       " 'friday august 6 2010',\n",
       " 'june 28 2002',\n",
       " 'friday february 25 2000',\n",
       " 'saturday february 10 2018',\n",
       " '8/3/99',\n",
       " 'july 9 1982',\n",
       " 'may 2 2005',\n",
       " 'monday august 6 1979',\n",
       " 'friday august 4 2017',\n",
       " '6 jul 1992',\n",
       " 'thursday july 12 2018',\n",
       " '7 may 2001',\n",
       " '3 jun 2017',\n",
       " 'tuesday july 11 1995',\n",
       " 'monday may 23 1994',\n",
       " 'thursday may 23 1996',\n",
       " 'friday february 19 1988',\n",
       " '6/7/94',\n",
       " '04.02.07',\n",
       " '06 feb 1976',\n",
       " 'saturday may 16 1992',\n",
       " '17 october 1987',\n",
       " 'tuesday january 30 1990',\n",
       " 'tuesday december 23 2014',\n",
       " 'mar 18 1977',\n",
       " '22.10.09',\n",
       " 'thursday june 22 1989',\n",
       " '29 jun 1997',\n",
       " '10 february 1970',\n",
       " 'saturday february 6 1988',\n",
       " 'thursday december 16 2010',\n",
       " 'monday march 30 1981',\n",
       " '29 december 1979',\n",
       " 'tuesday november 18 2014',\n",
       " '25 march 1998',\n",
       " 'january 5 1985',\n",
       " '4 august 1985',\n",
       " '25 sep 1972',\n",
       " '08 jul 1987',\n",
       " '11 02 72',\n",
       " 'friday july 9 1976',\n",
       " 'sunday august 5 1990',\n",
       " '25 apr 1972',\n",
       " 'sunday february 19 1984',\n",
       " 'wednesday july 29 1981',\n",
       " '12 apr 1971',\n",
       " '2/23/86',\n",
       " 'tuesday may 6 1980',\n",
       " 'saturday may 8 1993',\n",
       " 'wednesday september 18 2013',\n",
       " 'monday march 14 1977',\n",
       " '24.11.89',\n",
       " 'saturday september 30 1995',\n",
       " 'tuesday february 11 1986',\n",
       " 'june 24 1976',\n",
       " '10/4/72',\n",
       " 'february 3 1985',\n",
       " 'march 9 2015',\n",
       " 'september 27 2000',\n",
       " '04 dec 1971',\n",
       " 'monday october 11 1993',\n",
       " '13 november 1991',\n",
       " '14 sep 1980',\n",
       " 'wednesday august 21 1996',\n",
       " 'friday october 27 1989',\n",
       " 'thursday august 30 1984',\n",
       " '12 november 1970',\n",
       " 'friday february 17 1978',\n",
       " '16 may 1978',\n",
       " 'thursday january 21 2010',\n",
       " 'april 1 2005',\n",
       " '30 may 1973',\n",
       " '8 04 70',\n",
       " 'tuesday march 5 2013',\n",
       " 'monday february 9 1981',\n",
       " '30 march 1998',\n",
       " '28 04 72',\n",
       " 'friday november 10 1995',\n",
       " '18 jan 2011',\n",
       " '11 jun 1992',\n",
       " '4 jan 1980',\n",
       " '29 apr 2007',\n",
       " '27 dec 2009',\n",
       " '28 may 2018',\n",
       " 'august 15 2009',\n",
       " 'thursday september 20 1973',\n",
       " 'sunday february 9 1975',\n",
       " 'saturday december 20 2003',\n",
       " 'monday october 4 1999',\n",
       " '9 oct 1981',\n",
       " '18 june 2004',\n",
       " 'saturday october 30 2010',\n",
       " '10 july 2002',\n",
       " '08.03.87',\n",
       " 'saturday november 2 2013',\n",
       " 'november 4 1975',\n",
       " ...]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(columns=['matches','decoded','expected', 'input_sentence'])\n",
    "df_test.loc[0, ['matches','decoded', 'expected', 'input_sentence']]=[True, 'decoded_value','true_value', 'elem']\n",
    "for i, elem in enumerate(Xoh_test[0:10000]):\n",
    "    input_seq = elem.reshape(1,30,37)\n",
    "    decoded_value = decode_sequence(input_seq,np.where(oh_start_char==1)[0][0],encoder_model, decoder_model).replace('<end>','')\n",
    "    true_value = from_encode_to_chars(Y_test[i], reverse_target_char_index)\n",
    "    matches = decoded_value==true_value\n",
    "    df_test.loc[i, ['matches','decoded', 'expected', 'input_sentence']]=[matches, decoded_value,true_value,  from_oh_to_chars(elem, reverse_input_char_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     9809\n",
       "False     191\n",
       "Name: matches, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.matches.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study wrong cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'199<start><start><start><start><start><start><start><start>'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_new_phrase = from_nl_to_oh('9 may 1998', human_vocab, reverse_input_char_index, 30)\n",
    "oh_new_date = np.zeros((1,oh_new_phrase.shape[0], num_encoder_tokens))\n",
    "oh_new_date[0] = oh_new_phrase\n",
    "decode_sequence(oh_new_date, np.where(oh_start_char==1)[0][0],encoder_model, decoder_model) # '2018-05-23<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'199<start><start><start><start><start><start><start><start>'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_new_phrase = from_nl_to_oh('9 may 1989', human_vocab, reverse_input_char_index, 30)\n",
    "oh_new_date = np.zeros((1,oh_new_phrase.shape[0], num_encoder_tokens))\n",
    "oh_new_date[0] = oh_new_phrase\n",
    "decode_sequence(oh_new_date, np.where(oh_start_char==1)[0][0],encoder_model, decoder_model) # '2018-05-23<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Switching last two digits from years sometimes works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1987-05-09<end>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_new_phrase = from_nl_to_oh('9 may 1987', human_vocab, reverse_input_char_index, 30)\n",
    "oh_new_date = np.zeros((1,oh_new_phrase.shape[0], num_encoder_tokens))\n",
    "oh_new_date[0] = oh_new_phrase\n",
    "decode_sequence(oh_new_date, np.where(oh_start_char==1)[0][0],encoder_model, decoder_model) # '2018-05-23<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1978-05-09<end>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_new_phrase = from_nl_to_oh('9 may 1978', human_vocab, reverse_input_char_index, 30)\n",
    "oh_new_date = np.zeros((1,oh_new_phrase.shape[0], num_encoder_tokens))\n",
    "oh_new_date[0] = oh_new_phrase\n",
    "decode_sequence(oh_new_date, np.where(oh_start_char==1)[0][0],encoder_model, decoder_model) # '2018-05-23<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_keras]",
   "language": "python",
   "name": "conda-env-tensorflow_keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
