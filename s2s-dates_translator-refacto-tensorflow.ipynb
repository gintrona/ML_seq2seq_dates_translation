{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "\n",
    "#from keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \"\"\"one-hot encoding function\n",
    "    \"\"\"\n",
    "    \n",
    "    X=dataset['human']\n",
    "    Y = dataset['machine']\n",
    "    \n",
    "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
    "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
    "    \n",
    "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
    "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
    "\n",
    "    return X, np.array(Y), Xoh, Yoh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabularies and mapping between indices and characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_vocabulary=set()\n",
    "machine_vocabulary=set()\n",
    "t_data = data[0:10000] # get the first 10000 exemple for training\n",
    "\n",
    "for row in t_data.iterrows():\n",
    "    human_vocabulary.update(tuple(row[1][0]))\n",
    "    machine_vocabulary.update(tuple(row[1][1]))\n",
    "    \n",
    "human_vocab = dict(zip(sorted(human_vocabulary) + ['<unk>', '<pad>'], list(range(len(human_vocabulary) + 2))))\n",
    "inv_machine_vocab = dict(enumerate(sorted(machine_vocab)))\n",
    "machine_vocab = {v:k for k,v in inv_machine_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \n",
    "    X=dataset['human']\n",
    "    Y = dataset['machine']\n",
    "    \n",
    "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
    "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
    "    \n",
    "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
    "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
    "\n",
    "    return X, np.array(Y), Xoh, Yoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30) -> the maximum length of a phrase is set to 30\n",
      "Y.shape: (10000, 10) -> the output date in standardized format have lenght=10\n",
      "Xoh.shape: (10000, 30, 37) -> the human vocab contains 37 chars\n",
      "Yoh.shape: (10000, 10, 11) -> the machine vocab contains 11 chars\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data \n",
    "Tx = 30 # the maximum length of the input\n",
    "Ty = 10 # the length of the output\n",
    "X, Y, Xoh, Yoh = preprocess_data(t_data, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape, \"-> the maximum length of a phrase is set to 30\")\n",
    "print(\"Y.shape:\", Y.shape,  \"-> the output date in standardized format have lenght=10\")\n",
    "print(\"Xoh.shape:\", Xoh.shape, \"-> the human vocab contains 37 chars\")\n",
    "print(\"Yoh.shape:\", Yoh.shape,\"-> the machine vocab contains 11 chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "human_vocab =   {' ': 0,\n",
    "                 '.': 1,\n",
    "                 '/': 2,\n",
    "                 '0': 3,\n",
    "                 '1': 4,\n",
    "                 (...)\n",
    "                 '9': 12,\n",
    "                 'a': 13,\n",
    "                 'b': 14,\n",
    "                 (...)\n",
    "                 'v': 32,\n",
    "                 'w': 33,\n",
    "                 'y': 34,\n",
    "                 '<unk>': 35,\n",
    "                 '<pad>': 36}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "machine_vocab= {'-': 0,\n",
    "               '0': 1,\n",
    "               '1': 2,\n",
    "               '2': 3,\n",
    "               '3': 4,\n",
    "               '4': 5,\n",
    "               '5': 6,\n",
    "               '6': 7,\n",
    "               '7': 8,\n",
    "               '8': 9,\n",
    "               '9': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to chars \n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in human_vocab.items())\n",
    "\n",
    "# Add two additional chars to the dict: a start char and an end char\n",
    "reverse_target_char_index = {}\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in machine_vocab.items())\n",
    "reverse_target_char_index.update({11:'<start>', 12:'<end>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "reverse_target_char_index:\n",
    "{0: '-', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: '<start>', 12: '<end>'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Y_oh to build decoder_input_data and decoder_output_data from Y_oh\n",
    "num_samples,_,_ = Yoh.shape\n",
    "Tx_decoder = 11 # it's 10 (length of YYYY-MM-DD) plus additional char \n",
    "num_decoder_tokens = len(reverse_target_char_index) # returns 13\n",
    "decoder_input_data = np.zeros([num_samples, Tx_decoder, num_decoder_tokens])\n",
    "decoder_target_data = np.zeros([num_samples, Tx_decoder, num_decoder_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# encoder input data\n",
    "#####################\n",
    "encoder_input_data = Xoh\n",
    "\n",
    "####################\n",
    "# decoder_input_data\n",
    "####################\n",
    "# We build decoder_input_data from Y_oh by adding a start-of-sequence character \n",
    "# At timestep O : populate one-hot encoded sequences with start-of-sentence charcater\n",
    "oh_start_char = np.zeros(num_decoder_tokens)\n",
    "oh_start_char[11] = 1 # index 11 corresponds to '<start>'\n",
    "\n",
    "decoder_input_data[:, 0] = oh_start_char # all the exemples get a <start> char character\n",
    "\n",
    "for i in range(num_samples):\n",
    "    for j in range(0, Tx_decoder-1): # j=1,...10\n",
    "        for k in range(num_decoder_tokens - 2): # minus 2 because of the <start> ad <end> chars we added\n",
    "            decoder_input_data[i][j+1][k] = Yoh[i][j][k]\n",
    "            \n",
    "#####################\n",
    "# decoder_target_data\n",
    "#####################\n",
    "# We build decoder_target_data from Y_oh by adding a end-of-sequence character \n",
    "oh_end_char = np.zeros(num_decoder_tokens)\n",
    "oh_end_char[12] = 1 # index 12 corresponds to '<end>'\n",
    "decoder_target_data[:, Tx_decoder-1] = oh_end_char # targets get an <end> char in the last timestep\n",
    "\n",
    "for i in range(num_samples):\n",
    "    for j in range(0, Tx_decoder-1):\n",
    "        for k in range(num_decoder_tokens):\n",
    "            # decoder_target_data is one time step ahead of the decoder_input_data\n",
    "            decoder_target_data[i][j][k] = decoder_input_data[i][j+1][k] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_oh_to_chars(matrix, reverse_dictionary):\n",
    "    # Take a one-hot enconding two dimensional ndarray and\n",
    "    # translate it back to human language (phrase)\n",
    "    tx, dim = matrix.shape\n",
    "    resu = str()\n",
    "    for i in range(tx):\n",
    "            if len(np.where(matrix[i]==1)[0])==0:\n",
    "                break\n",
    "            else:\n",
    "                index = np.where(matrix[i]==1)[0][0]\n",
    "                if reverse_dictionary[index]!='<pad>':\n",
    "                    resu+=(reverse_dictionary[index])\n",
    "                else:\n",
    "                    break\n",
    "    return resu\n",
    "\n",
    "#print(encoder_input_data[1],'\\n', from_oh_to_chars(encoder_input_data[1], reverse_input_char_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  2  5 10  2 10 12 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]  \n",
      "is decoded to \n",
      " 1/27/79\n"
     ]
    }
   ],
   "source": [
    "def from_encode_to_chars(vector_indices, reverse_input_char_index):\n",
    "    resu = ''   \n",
    "    for _,index in enumerate(vector_indices):\n",
    "        if reverse_input_char_index[index] != '<pad>':\n",
    "            resu+=reverse_input_char_index[index]\n",
    "        else:\n",
    "            break\n",
    "    return resu\n",
    "\n",
    "print(X[0], \" \\nis decoded to \\n\", from_encode_to_chars(X[0], reverse_input_char_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_string(phrase, human_vocab, Tx):\n",
    "    # take string and return a list with the index of each char (in the dictionary human vocab)\n",
    "    # take phrase 'foo' and return [18,26,26]\n",
    "    resu = np.zeros(Tx, dtype=np.int8)\n",
    "    for idx,char in enumerate(phrase):\n",
    "        resu[idx] = human_vocab.get(char)\n",
    "        idx=idx+1\n",
    "    resu[idx:] = human_vocab.get('<pad>')\n",
    "    return resu\n",
    "\n",
    "def from_encode_to_oh(encoded_phrase_indices, reverse_input_char_index):\n",
    "    cols = encoded_phrase_indices\n",
    "    matrix = np.zeros((len(encoded_phrase_indices) , len(reverse_input_char_index) ) )\n",
    "    matrix[np.arange(len(encoded_phrase_indices)) ,cols ] = 1\n",
    "    return matrix\n",
    "\n",
    "def from_nl_to_oh(phrase,human_vocab,reverse_input_char_index, Tx):\n",
    "    encoded_phrase_indices = encode_string(phrase, human_vocab, Tx)\n",
    "    resu = from_encode_to_oh(encoded_phrase_indices,reverse_input_char_index)\n",
    "    return resu\n",
    "\n",
    "from_nl_to_oh('20 may 1998', human_vocab, reverse_input_char_index, Tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/27/79\n",
      "<start>1979-01-27\n",
      "1979-01-27<end>\n"
     ]
    }
   ],
   "source": [
    "print(from_encode_to_chars(X[0], reverse_input_char_index))\n",
    "print(from_oh_to_chars(decoder_input_data[0], reverse_target_char_index)) # '<start>1998-05-09'\n",
    "print(from_oh_to_chars(decoder_target_data[0], reverse_target_char_index)) # '1998-05-09<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: tuesday november 6 2012\n",
      "<start>2012-11-06\n",
      "2012-11-06<end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Example:\", from_encode_to_chars(X[1], reverse_input_char_index))\n",
    "print(from_oh_to_chars(decoder_input_data[1], reverse_target_char_index)) # '<start>1998-05-09'\n",
    "print(from_oh_to_chars(decoder_target_data[1], reverse_target_char_index)) # '1998-05-09<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuesday august 20 2013\n",
      "<start>2013-08-20\n",
      "2013-08-20<end>\n"
     ]
    }
   ],
   "source": [
    "print(from_encode_to_chars(X[5], reverse_input_char_index))\n",
    "print(from_oh_to_chars(decoder_input_data[5], reverse_target_char_index)) # '<start>1998-05-09'\n",
    "print(from_oh_to_chars(decoder_target_data[5], reverse_target_char_index)) # '1998-05-09<end>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define here some metaparams\n",
    "batch_size = 64 # 64  # Batch size for training.\n",
    "epochs =  50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "num_encoder_tokens = len(human_vocab)\n",
    "num_decoder_tokens = num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define here the building blocks of our models\n",
    "lstm_encoder_layer = LSTM(latent_dim, return_state=True, name='lstm_encoder')\n",
    "lstm_decoder_layer =  LSTM(latent_dim, return_sequences=True, return_state=True, name='lstm_decoder')\n",
    "dense_layer = Dense(num_decoder_tokens, activation='softmax', name=\"decoder_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Define model for training\n",
    "##########################\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "def get_training_model(lstm_encoder_layer, lstm_decoder_layer, dense_layer):\n",
    "    \n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_inputs')\n",
    "    \n",
    "    # The input sequence is encoded; the resulting state vectors are kept; \n",
    "    # encoder_outputs, state_h, state_c are tensors\n",
    "    encoder_outputs, state_h, state_c = lstm_encoder_layer(encoder_inputs)\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "    \"\"\"\n",
    "    We set our decoder up to return full output sequences,\n",
    "    and to return internal states as well. We don't use the\n",
    "    return states in the training model, but we will use them \n",
    "    in the inference phase.\n",
    "    \"\"\"\n",
    "    decoder_outputs_0, _, _ = lstm_decoder_layer(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_outputs_1 = dense_layer(decoder_outputs_0)\n",
    "    \n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` and `decoder_input_data` into `decoder_target_data`\n",
    "    training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs_1)\n",
    "\n",
    "    return training_model\n",
    "\n",
    "training_model = get_training_model(lstm_encoder_layer, lstm_decoder_layer, dense_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"training_model.input: \\n\" , training_model.input,\"\\n\")\n",
    "print(\"training_model.output: \\n\", training_model.output, \"\\n\")\n",
    "\n",
    "training_model.input: \n",
    " [<tf.Tensor 'encoder_inputs_5:0' shape=(?, ?, 37) dtype=float32>, <tf.Tensor 'input_6:0' shape=(?, ?, 13) dtype=float32>] \n",
    "\n",
    "training_model.output: \n",
    " Tensor(\"decoder_dense_5/truediv:0\", shape=(?, ?, 13), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'lstm_decoder',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'return_sequences': True,\n",
       " 'return_state': True,\n",
       " 'go_backwards': False,\n",
       " 'stateful': False,\n",
       " 'unroll': False,\n",
       " 'units': 256,\n",
       " 'activation': 'tanh',\n",
       " 'recurrent_activation': 'hard_sigmoid',\n",
       " 'use_bias': True,\n",
       " 'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "  'config': {'scale': 1.0,\n",
       "   'mode': 'fan_avg',\n",
       "   'distribution': 'uniform',\n",
       "   'seed': None,\n",
       "   'dtype': 'float32'}},\n",
       " 'recurrent_initializer': {'class_name': 'Orthogonal',\n",
       "  'config': {'gain': 1.0, 'seed': None, 'dtype': 'float32'}},\n",
       " 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}},\n",
       " 'unit_forget_bias': True,\n",
       " 'kernel_regularizer': None,\n",
       " 'recurrent_regularizer': None,\n",
       " 'bias_regularizer': None,\n",
       " 'activity_regularizer': None,\n",
       " 'kernel_constraint': None,\n",
       " 'recurrent_constraint': None,\n",
       " 'bias_constraint': None,\n",
       " 'dropout': 0.0,\n",
       " 'recurrent_dropout': 0.0,\n",
       " 'implementation': 1}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_decoder_layer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None, 37)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, None, 13)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_encoder (LSTM)             [(None, 256), (None, 301056      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_decoder (LSTM)             [(None, None, 256),  276480      input_6[0][0]                    \n",
      "                                                                 lstm_encoder[5][1]               \n",
      "                                                                 lstm_encoder[5][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 13)     3341        lstm_decoder[5][0]               \n",
      "==================================================================================================\n",
      "Total params: 580,877\n",
      "Trainable params: 580,877\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Run training\n",
    "##############\n",
    "TRAINING = False\n",
    "if TRAINING:\n",
    "    training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "    validation_split=0.2)\n",
    "    # Save model\n",
    "    training_model.save('s2s_dates_translator.h5')\n",
    "else:\n",
    "    training_model.load_weights('s2s_dates_translator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Voici the steps:\n",
    "# 1) encode input and retain output as initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "def sampling_model(latent_dim, lstm_encoder_layer, lstm_decoder_layer,decoder_dense):\n",
    "    # Define sampling models\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_inputs')\n",
    "    encoder_outputs, state_h, state_c = lstm_encoder_layer(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and keep only the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,), name='decoder_state_input_h')\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,), name='decoder_state_input_c')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    # This is the lstm we defined before \n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "    decoder_outputs, state_h, state_c = lstm_decoder_layer(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs, # the input is a list containing three tensors\n",
    "        [decoder_outputs] + decoder_states) # the input is also a list containing tensors\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "encoder_model, decoder_model = sampling_model(latent_dim, lstm_encoder_layer, lstm_decoder_layer, dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, index_start_char,encoder_model, decoder_model):\n",
    "    # Encode the input sequence as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, index_start_char] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    decoded_sentence = ''\n",
    "    for char_elem in range(11) :\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "  \n",
    "        # Sample a token wih argmax\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        # Get char associatedwith token\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states. Here we reassign the initial decoder states.  \n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input date: 1/27/79\n",
      "Predicted date 1979-01-27<end>\n",
      "Expected result is 1979-01-27<end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input date:\", from_oh_to_chars(encoder_input_data[0], reverse_input_char_index))\n",
    "print(\"Predicted date\", decode_sequence(encoder_input_data[0:1], np.where(oh_start_char==1)[0][0], encoder_model, decoder_model)) # '1990-04-28<end>'\n",
    "print(\"Expected result is\", from_oh_to_chars(decoder_target_data[0], reverse_target_char_index)) # '1990-04-28<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input date: thursday december 29 1977\n",
      "Predicted date 1977-12-29<end>\n",
      "Expected result is 1977-12-29<end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input date:\", from_oh_to_chars(encoder_input_data[2], reverse_input_char_index))\n",
    "print(\"Predicted date\", decode_sequence(encoder_input_data[2:3], np.where(oh_start_char==1)[0][0], encoder_model, decoder_model)) # '1990-04-28<end>'\n",
    "print(\"Expected result is\", from_oh_to_chars(decoder_target_data[2], reverse_target_char_index)) # '1990-04-28<end>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['matches','decoded','expected', 'input_sentence'])\n",
    "df.loc[0, ['matches','decoded', 'expected', 'input_sentence']]=[True, 'decoded_value','true_value', 'elem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with data in the triaining set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: 1/27/79\n",
      "Decoded sentence: 1979-01-27<end> (should be  1979-01-27 )\n",
      "Input sentence: tuesday november 6 2012\n",
      "Decoded sentence: 2012-11-06<end> (should be  2012-11-06 )\n",
      "Input sentence: thursday december 29 1977\n",
      "Decoded sentence: 1977-12-29<end> (should be  1977-12-29 )\n",
      "Input sentence: 20 may 2008\n",
      "Decoded sentence: 2008-05-20<end> (should be  2008-05-20 )\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(4):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq,np.where(oh_start_char==1)[0][0],encoder_model, decoder_model)\n",
    "\n",
    "    print('Input sentence:', t_data.iloc[seq_index]['human'])\n",
    "    print('Decoded sentence:', decoded_sentence , '(should be ', t_data.iloc[seq_index]['machine'], ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, elem in enumerate(encoder_input_data[0:10000]):\n",
    "    input_seq = elem.reshape(1,30,37)\n",
    "    decoded_value = decode_sequence(input_seq,np.where(oh_start_char==1)[0][0],encoder_model, decoder_model)\n",
    "    true_value = from_oh_to_chars(decoder_target_data[i], reverse_target_char_index)\n",
    "    matches = decoded_value==true_value\n",
    "    df.loc[i, ['matches','decoded', 'expected', 'input_sentence']]=[matches, decoded_value,true_value,  from_oh_to_chars(elem, reverse_input_char_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     9799\n",
       "False     201\n",
       "Name: matches, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.matches.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[10001:20000]\n",
    "Tx = 30\n",
    "Ty = 10\n",
    "X_test, Y_test, Xoh_test, Yoh_test = preprocess_data(test_data, human_vocab, machine_vocab, Tx, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984-11-15\n",
      "11/15/84\n"
     ]
    }
   ],
   "source": [
    "# Check dates\n",
    "i=np.random.randint(0,10000)\n",
    "print(from_encode_to_chars(Y_test[i], reverse_target_char_index))\n",
    "print(from_encode_to_chars(X_test[i], reverse_input_char_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this dataset to check whether the test date is already in the input dataset\n",
    "nl_input_data= [from_encode_to_chars(X_test[i], reverse_input_char_index).replace('<pad>','') for i in range(0,9999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_date_to_oh(date_nl, nl_input_data):\n",
    "    # look if date in argument is already present in training set\n",
    "    if date_nl.replace('<pad>','') in nl_input_data:\n",
    "        raise ValueError('date is already in input dataset ')\n",
    "    \n",
    "    oh_new_phrase = from_nl_to_oh(date_nl, human_vocab, reverse_input_char_index, 30)\n",
    "    oh_new_date = np.zeros((1,oh_new_phrase.shape[0], num_encoder_tokens))\n",
    "    oh_new_date[0] = oh_new_phrase\n",
    "    return oh_new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(columns=['matches','decoded','expected', 'input_sentence'])\n",
    "df_test.loc[0, ['matches','decoded', 'expected', 'input_sentence']]=[True, 'decoded_value','true_value', 'elem']\n",
    "for i, elem in enumerate(Xoh_test[0:10000]):\n",
    "    input_seq = elem.reshape(1,30,37)\n",
    "    decoded_value = decode_sequence(input_seq,np.where(oh_start_char==1)[0][0],encoder_model, decoder_model).replace('<end>','')\n",
    "    true_value = from_encode_to_chars(Y_test[i], reverse_target_char_index)\n",
    "    matches = decoded_value==true_value\n",
    "    df_test.loc[i, ['matches','decoded', 'expected', 'input_sentence']]=[matches, decoded_value,true_value,  from_oh_to_chars(elem, reverse_input_char_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     9798\n",
       "False     201\n",
       "Name: matches, dtype: int64"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.matches.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making prediction without single sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004-11-11<end>\n",
      "2004-11-18<end>\n",
      "Input date: 18 november 2004\n"
     ]
    }
   ],
   "source": [
    "index = 16 # 7,8, 11, 16 wrong\n",
    "states_v = encoder_model.predict(encoder_input_data[index:index+1])\n",
    "target_seq = np.zeros((1, 11, num_decoder_tokens))\n",
    "target_seq[0, 0, np.where(oh_start_char==1)[0][0]] = 1.\n",
    "outs = decoder_model.predict([target_seq]+states_v)\n",
    "\n",
    "argmax_indices = list(map(np.argmax, outs[0][0]))\n",
    "sampled_chars = list(map(reverse_target_char_index.get, argmax_indices))\n",
    "print(''.join(sampled_chars))\n",
    "print(decode_sequence(encoder_input_data[index:index+1], np.where(oh_start_char==1)[0][0],encoder_model, decoder_model))\n",
    "print(\"Input date:\", from_oh_to_chars(encoder_input_data[index], reverse_input_char_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study wrong cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'199<start><start><start><start><start><start><start><start>'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_new_phrase = from_nl_to_oh('9 may 1998', human_vocab, reverse_input_char_index, 30)\n",
    "oh_new_date = np.zeros((1,oh_new_phrase.shape[0], num_encoder_tokens))\n",
    "oh_new_date[0] = oh_new_phrase\n",
    "decode_sequence(oh_new_date, np.where(oh_start_char==1)[0][0],encoder_model, decoder_model) # '2018-05-23<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'199<start><start><start><start><start><start><start><start>'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_new_phrase = from_nl_to_oh('9 may 1989', human_vocab, reverse_input_char_index, 30)\n",
    "oh_new_date = np.zeros((1,oh_new_phrase.shape[0], num_encoder_tokens))\n",
    "oh_new_date[0] = oh_new_phrase\n",
    "decode_sequence(oh_new_date, np.where(oh_start_char==1)[0][0],encoder_model, decoder_model) # '2018-05-23<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Switching last two digits from years sometimes works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1987-05-09<end>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_new_phrase = from_nl_to_oh('9 may 1987', human_vocab, reverse_input_char_index, 30)\n",
    "oh_new_date = np.zeros((1,oh_new_phrase.shape[0], num_encoder_tokens))\n",
    "oh_new_date[0] = oh_new_phrase\n",
    "decode_sequence(oh_new_date, np.where(oh_start_char==1)[0][0],encoder_model, decoder_model) # '2018-05-23<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1978-05-09<end>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_new_phrase = from_nl_to_oh('9 may 1978', human_vocab, reverse_input_char_index, 30)\n",
    "oh_new_date = np.zeros((1,oh_new_phrase.shape[0], num_encoder_tokens))\n",
    "oh_new_date[0] = oh_new_phrase\n",
    "decode_sequence(oh_new_date, np.where(oh_start_char==1)[0][0],encoder_model, decoder_model) # '2018-05-23<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_keras]",
   "language": "python",
   "name": "conda-env-tensorflow_keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
